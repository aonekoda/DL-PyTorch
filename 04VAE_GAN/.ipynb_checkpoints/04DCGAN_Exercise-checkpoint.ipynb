{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional GANs\n",
    "\n",
    "[구글 Colab에서 실행하기](https://colab.research.google.com/github/aonekoda/DL-PyTorch/blob/master/04VAE_GAN/04DCGAN_Exercise.ipynb)\n",
    "\n",
    "이번 실습에서는 generator 와 discriminator에 convolutional layers를 사용하는 GAN 모형을 생성한다. 이것을 DCGAN 이라고 한다. \n",
    "\n",
    "\n",
    "DCGAN 을 [Street View House Numbers](http://ufldl.stanford.edu/housenumbers/) (SVHN) dataset에 훈련하여 집 주소 이미지를 생성한다. 앞 서 실습한 MNIST보다는 생성해야 할 이미지가 복잡하다. \n",
    "\n",
    "![img](../assets/svhn_dcgan.png)\n",
    "\n",
    "DCGAN 을 사용하여 그럴듯한 images 를 생성한다. \n",
    "* Load in and pre-process the house numbers dataset\n",
    "* Define discriminator and generator networks\n",
    "* Train these adversarial networks\n",
    "* Visualize the loss over time and some sample, generated images\n",
    "\n",
    "#### Deeper Convolutional Networks\n",
    "\n",
    "SVHN은 MNIST data에 비해 복잡하다. 따라서 여러개의 convolutional 과 transpose convolutional layers 를 사용하여  discriminator와 generator를 구현한다. batch normalization기법도 사용된다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "SVHN dataset을 다운로드한다. Tensor datatypes으로 적절히 transform 하고 적절한 batch size를 가지는 dataloaders 로 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "# Tensor transform\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# SVHN training datasets\n",
    "svhn_train = datasets.SVHN(root='data/', split='train', download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 0\n",
    "\n",
    "# build DataLoaders for SVHN dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=svhn_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data\n",
    "\n",
    "각각의 이미지는 32x32 사이즈의 3 color channels (RGB)이미지이다. 각각의 이미지에는 숫자 label이 붙어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "plot_size=20\n",
    "for idx in np.arange(plot_size):\n",
    "    ax = fig.add_subplot(2, plot_size/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: scaling from -1 to 1\n",
    "\n",
    "pre-processing; DCGAN은 `tanh` activated generator를 사용하므로 픽셀 값을 -1 to 1로 rescale한다. 처음 읽어들이면 0-1 사이 값으로 되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current range\n",
    "img = images[0]\n",
    "\n",
    "print('Min: ', img.min())\n",
    "print('Max: ', img.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper scale function\n",
    "def scale(x, feature_range=(-1, 1)):\n",
    "    ''' Scale takes in an image x and returns that image, scaled\n",
    "       with a feature_range of pixel values from -1 to 1. \n",
    "       This function assumes that the input x is already scaled from 0-1.'''\n",
    "    # assume x is scaled to (0, 1)\n",
    "    # scale to feature_range and return scaled x\n",
    "    # TODO \n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled range\n",
    "scaled_img = scale(img)\n",
    "\n",
    "print('Scaled min: ', scaled_img.min())\n",
    "print('Scaled max: ', scaled_img.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "\n",
    "maxpooling layers를 사용하지 않는다. \n",
    "* discriminator 의 input은 32x32x3 tensor images\n",
    "* convolutional, hidden layers를 사용한다.\n",
    "* output은 fully connected layer이다; sigmoid output를 사용하지 않고 loss function으로 [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss)를 사용한다.\n",
    "\n",
    "\n",
    "![img](../assets/conv_discriminator.png)\n",
    "\n",
    "convolutional layers 의 depth를 32 -> 64 ->128...로 늘려나간다. maxpooling layers을 사용하지 않고 stride를 사용하여 down-sample을 수행한다.\n",
    "\n",
    "batch normalization 을 사용한다. [nn.BatchNorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d) **다만 first convolutional layer 와 final에서는 사용하지 않는다**. \n",
    "\n",
    "#### Helper `conv` function \n",
    "\n",
    "convolution > batch norm > leaky ReLU를 반복적으로 사용하여 layer를 구성하므로 별도의 helper 함수를 만들어 간단하게 사용한다. convolutional + optional batch norm layer로 이루어진 sequential series로 모형을 구성한다 . \n",
    "\n",
    "\n",
    "**kernel_size = 4** , **stride = 2** 를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# helper conv function\n",
    "def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
    "    \"\"\"Creates a convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, \n",
    "                           kernel_size, stride, padding, bias=False)\n",
    "    \n",
    "    # append conv layer\n",
    "    layers.append(conv_layer)\n",
    "\n",
    "    if batch_norm:\n",
    "        # append batchnorm layer\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "     \n",
    "    # using Sequential container\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_dim=32):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # complete init function\n",
    "        \n",
    "        #TODO \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # complete forward function\n",
    "        \n",
    "        \n",
    "        #TODO \n",
    "        \n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "G의 input 은 noise vector `z`이다. ouput은 우리가 생성해야할 32X32X3의 이미지이다.\n",
    "\n",
    "![img](../assets/conv_generator.png)\n",
    "\n",
    "\n",
    "transpose convolutional layers 를 사용하여 images를 생성한다. \n",
    "\n",
    "* 첫 layer는 fully connected layer이다. 4x4x512 같은 w,h가 작고 depth가 큰 형태로 시작한다. \n",
    "* batch normalization과 leaky ReLU activation를 사용한다. \n",
    "* [transpose convolutional layers](https://pytorch.org/docs/stable/nn.html#convtranspose2d)을 사용하여 w, h를 더블로 늘려나가고 depth는 줄여서 원하는 이미지 사이즈로 생성한다.\n",
    "* batch normalization 과 ReLU 를 모든 hidden layers에 사용하는데 마지막은  `tanh` activation을 사용한다.\n",
    "\n",
    "#### Helper `deconv` function\n",
    "\n",
    "transpose convolution > batch norm > ReLU의 반복으로 D를 구성한다. 이를 손쉽게 하기 위해 helper함수를 하나 만든다.\n",
    "\n",
    "\n",
    "Note: transpose convolutions에서 **kernel_size = 4** and a **stride = 2**로 설정한다 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper deconv function\n",
    "def deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
    "    \"\"\"Creates a transposed-convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    ## TODO: Complete this function\n",
    "    ## create a sequence of transpose + optional batch norm layers\n",
    "\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_size, conv_dim=32):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # complete init function\n",
    "        # TODO\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # complete forward function\n",
    "        #TODO \n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build complete network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparams\n",
    "conv_dim = 32\n",
    "z_size = 100\n",
    "\n",
    "# define discriminator and generator\n",
    "D = Discriminator(conv_dim)\n",
    "G = Generator(z_size=z_size, conv_dim=conv_dim)\n",
    "\n",
    "print(D)\n",
    "print()\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if train_on_gpu:\n",
    "    # move models to GPU\n",
    "    G.cuda()\n",
    "    D.cuda()\n",
    "    print('GPU available for training. Models moved to GPU')\n",
    "else:\n",
    "    print('Training on CPU.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Discriminator and Generator Losses\n",
    "\n",
    "앞 선 예제에서와 같이 D와 G의 loss함수를 정한다.\n",
    "\n",
    "### Discriminator Losses\n",
    "\n",
    "> * discriminator에서 total loss 는  real 와 fake images의 합이다. `d_loss = d_real_loss + d_fake_loss`. \n",
    "* discriminator는 real images에 대해서 output 1, fake images에 대해서는 0 을 출력한다.\n",
    "\n",
    "* `D(real_images) = 1`. \n",
    "* `D(fake_images) = 0`, \n",
    "* `fake_images = G(z)`. \n",
    "\n",
    "### Generator Loss\n",
    "\n",
    "G는`D(fake_images) = 1`이 되어야 한다. 즉 generates (fakes)가 real로 판별되도록 해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_loss(D_out, smooth=False):\n",
    "    batch_size = D_out.size(0)\n",
    "    # label smoothing\n",
    "    if smooth:\n",
    "        # smooth, real labels = 0.9\n",
    "        labels = torch.ones(batch_size)*0.9\n",
    "    else:\n",
    "        labels = torch.ones(batch_size) # real labels = 1\n",
    "    # move labels to GPU if available     \n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    # binary cross entropy with logits loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "def fake_loss(D_out):\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.zeros(batch_size) # fake labels = 0\n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "연구에 따르면 DCGAN model 에서는 learning_rate를 작게 설정한다.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "GANs은 hyperparameters 설정에 민감하다. 이에 대해서는 [DCGAN paper](https://arxiv.org/pdf/1511.06434.pdf) 을 참조한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# params\n",
    "# TODO\n",
    "\n",
    "lr = \n",
    "beta1=  # momentum decay\n",
    "beta2=  # scale decay\n",
    "\n",
    "# Create optimizers for the discriminator and generator\n",
    "d_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2])\n",
    "g_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "discriminator 와 the generator를 번갈아 가며 훈련한다.  `real_loss` 와 `fake_loss` 를 사용하여 훈련한다.\n",
    "\n",
    "### Discriminator training\n",
    "1. real(훈련 데이터)에 대해서 discriminator loss 를 계산        \n",
    "2. fake images를 생성\n",
    "3. fake(generated image)에 대한discriminator loss 를 구한다.     \n",
    "4. total lpss = real logg + fake loss\n",
    "5. backpropagation + optimization을 수행하여 discriminator의 weights를 훈련한다.\n",
    "\n",
    "### Generator training\n",
    "1. fake images를 생성한다.\n",
    "2. fake에 대한 discriminator loss 를 계산한다. D(fake_image) = 1\n",
    "3. backpropagation + optimization step 을 수행하여 generator의 weights를 훈련한다.\n",
    "\n",
    "#### Saving Samples\n",
    "\n",
    "loss statistics 와 generated \"fake\" samples을 저장한다.\n",
    "\n",
    "**Evaluation mode**\n",
    "\n",
    "generator 가 sample 이미지를 생성할 때는 evaluation mode를 사용한다: `G.eval()`. evaluation 모드에서는 batch normalization layers가 training에 사용한 평균과 분산을 사용하지 않는다. 그리고 dropout layers도 off된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "# training hyperparams\n",
    "num_epochs = 30\n",
    "\n",
    "# keep track of loss and generated, \"fake\" samples\n",
    "samples = []\n",
    "losses = []\n",
    "\n",
    "print_every = 300\n",
    "\n",
    "# Get some fixed data for sampling. These are images that are held\n",
    "# constant throughout training, and allow us to inspect the model's performance\n",
    "sample_size=16\n",
    "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
    "fixed_z = torch.from_numpy(fixed_z).float()\n",
    "\n",
    "# train the network\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch_i, (real_images, _) in enumerate(train_loader):\n",
    "                \n",
    "        batch_size = real_images.size(0)\n",
    "        \n",
    "        # important rescaling step\n",
    "        real_images = scale(real_images)\n",
    "        \n",
    "        # ============================================\n",
    "        #            TRAIN THE DISCRIMINATOR\n",
    "        # ============================================\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Train with real images\n",
    "\n",
    "        # Compute the discriminator losses on real images \n",
    "        if train_on_gpu:\n",
    "            real_images = real_images.cuda()\n",
    "        \n",
    "        D_real = D(real_images)\n",
    "        d_real_loss = real_loss(D_real)\n",
    "        \n",
    "        # 2. Train with fake images\n",
    "        \n",
    "        # Generate fake images\n",
    "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "        z = torch.from_numpy(z).float()\n",
    "        # move x to GPU, if available\n",
    "        if train_on_gpu:\n",
    "            z = z.cuda()\n",
    "        fake_images = G(z)\n",
    "        \n",
    "        # Compute the discriminator losses on fake images            \n",
    "        D_fake = D(fake_images)\n",
    "        d_fake_loss = fake_loss(D_fake)\n",
    "        \n",
    "        # add up loss and perform backprop\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        \n",
    "        # =========================================\n",
    "        #            TRAIN THE GENERATOR\n",
    "        # =========================================\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Train with fake images and flipped labels\n",
    "        \n",
    "        # Generate fake images\n",
    "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "        z = torch.from_numpy(z).float()\n",
    "        if train_on_gpu:\n",
    "            z = z.cuda()\n",
    "        fake_images = G(z)\n",
    "        \n",
    "        # Compute the discriminator losses on fake images \n",
    "        # using flipped labels!\n",
    "        D_fake = D(fake_images)\n",
    "        g_loss = real_loss(D_fake) # use real loss to flip labels\n",
    "        \n",
    "        # perform backprop\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Print some loss stats\n",
    "        if batch_i % print_every == 0:\n",
    "            # append discriminator loss and generator loss\n",
    "            losses.append((d_loss.item(), g_loss.item()))\n",
    "            # print discriminator and generator loss\n",
    "            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "    \n",
    "    ## AFTER EACH EPOCH##    \n",
    "    # generate and save sample, fake images\n",
    "    G.eval() # for generating samples\n",
    "    if train_on_gpu:\n",
    "        fixed_z = fixed_z.cuda()\n",
    "    samples_z = G(fixed_z)\n",
    "    samples.append(samples_z)\n",
    "    G.train() # back to training mode\n",
    "\n",
    "\n",
    "# Save training generator samples\n",
    "with open('train_samples.pkl', 'wb') as f:\n",
    "    pkl.dump(samples, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator', alpha=0.5)\n",
    "plt.plot(losses.T[1], label='Generator', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generator samples from training\n",
    "\n",
    "generator가 생성한 이미지를 화면에 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for viewing a list of passed in sample images\n",
    "def view_samples(epoch, samples):\n",
    "    fig, axes = plt.subplots(figsize=(16,4), nrows=2, ncols=8, sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
    "        img = img.detach().cpu().numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = ((img +1)*255 / (2)).astype(np.uint8) # rescale to pixel range (0-255)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        im = ax.imshow(img.reshape((32,32,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = view_samples(-1, samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
